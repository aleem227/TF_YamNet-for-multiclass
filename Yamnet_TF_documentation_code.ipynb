{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from IPython import display\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_io as tfio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Yamnet Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "yamnet_model_handle = 'https://tfhub.dev/google/yamnet/1'\n",
    "yamnet_model = hub.load(yamnet_model_handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Audio Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions for loading audio files and making sure the sample rate is correct.\n",
    "\n",
    "@tf.function\n",
    "def load_wav_16k_mono(filename):\n",
    "    \"\"\" Load a WAV file, convert it to a float tensor, resample to 16 kHz single-channel audio. \"\"\"\n",
    "    file_contents = tf.io.read_file(filename)\n",
    "    wav, sample_rate = tf.audio.decode_wav(\n",
    "          file_contents,\n",
    "          desired_channels=1)\n",
    "    wav = tf.squeeze(wav, axis=-1)\n",
    "    sample_rate = tf.cast(sample_rate, dtype=tf.int64)\n",
    "    wav = tfio.audio.resample(wav, rate_in=sample_rate, rate_out=16000)\n",
    "    return wav"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Loading (Custom Samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>book_page_flip\\a5-book-flipping-02-55290_01_Ad...</td>\n",
       "      <td>book_page_flip</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>book_page_flip\\a5-book-flipping-02-55290_01_Ad...</td>\n",
       "      <td>book_page_flip</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>book_page_flip\\a5-book-flipping-02-55290_01_Ga...</td>\n",
       "      <td>book_page_flip</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>book_page_flip\\a5-book-flipping-02-55290_01_Pi...</td>\n",
       "      <td>book_page_flip</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>book_page_flip\\a5-book-flipping-02-55290_01_Pi...</td>\n",
       "      <td>book_page_flip</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7695</th>\n",
       "      <td>Vacuum_cleaner\\Vacuum cleaner_516_aug_PitchShi...</td>\n",
       "      <td>Vacuum_cleaner</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7696</th>\n",
       "      <td>Vacuum_cleaner\\Vacuum cleaner_516_aug_PitchShi...</td>\n",
       "      <td>Vacuum_cleaner</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7697</th>\n",
       "      <td>Vacuum_cleaner\\Vacuum cleaner_516_aug_PitchShi...</td>\n",
       "      <td>Vacuum_cleaner</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7698</th>\n",
       "      <td>Vacuum_cleaner\\Vacuum cleaner_516_aug_PitchShi...</td>\n",
       "      <td>Vacuum_cleaner</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7699</th>\n",
       "      <td>Vacuum_cleaner\\Vacuum cleaner_516_aug_PitchShi...</td>\n",
       "      <td>Vacuum_cleaner</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7700 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               filename        category\n",
       "0     book_page_flip\\a5-book-flipping-02-55290_01_Ad...  book_page_flip\n",
       "1     book_page_flip\\a5-book-flipping-02-55290_01_Ad...  book_page_flip\n",
       "2     book_page_flip\\a5-book-flipping-02-55290_01_Ga...  book_page_flip\n",
       "3     book_page_flip\\a5-book-flipping-02-55290_01_Pi...  book_page_flip\n",
       "4     book_page_flip\\a5-book-flipping-02-55290_01_Pi...  book_page_flip\n",
       "...                                                 ...             ...\n",
       "7695  Vacuum_cleaner\\Vacuum cleaner_516_aug_PitchShi...  Vacuum_cleaner\n",
       "7696  Vacuum_cleaner\\Vacuum cleaner_516_aug_PitchShi...  Vacuum_cleaner\n",
       "7697  Vacuum_cleaner\\Vacuum cleaner_516_aug_PitchShi...  Vacuum_cleaner\n",
       "7698  Vacuum_cleaner\\Vacuum cleaner_516_aug_PitchShi...  Vacuum_cleaner\n",
       "7699  Vacuum_cleaner\\Vacuum cleaner_516_aug_PitchShi...  Vacuum_cleaner\n",
       "\n",
       "[7700 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Define input folder containing the preprocessed audio files\n",
    "input_folder = r'AudioSet\\new_created'\n",
    "\n",
    "# Initialize lists to store data\n",
    "file_paths = []\n",
    "class_names = []\n",
    "\n",
    "# Define the maximum number of files per category\n",
    "max_files_per_category = 700\n",
    "\n",
    "# Iterate over each file in the input folder\n",
    "for file_name in os.listdir(input_folder):\n",
    "    folder_path = os.path.join(input_folder, file_name)\n",
    "    \n",
    "    # Check if the item is a directory\n",
    "    if os.path.isdir(folder_path):\n",
    "        class_name = file_name  # Use folder name as class name\n",
    "        file_count = 0  # Counter for the number of files per category\n",
    "        \n",
    "        # Iterate over files in the directory\n",
    "        for file in os.listdir(folder_path):\n",
    "            if file.endswith(\".wav\"):\n",
    "                file_path = os.path.join(file_name, file)  # Changed to only file_name\n",
    "                \n",
    "                # Append file path and class name to lists\n",
    "                file_paths.append(file_path)\n",
    "                class_names.append(class_name)\n",
    "                \n",
    "                file_count += 1\n",
    "                if file_count >= max_files_per_category:\n",
    "                    break  # Break the loop if maximum files per category reached\n",
    "\n",
    "# Create DataFrame from the collected data\n",
    "preprocessed_df = pd.DataFrame({\n",
    "    'filename': file_paths,\n",
    "    'category': class_names\n",
    "})\n",
    "\n",
    "# Display DataFrame\n",
    "pd_data = preprocessed_df\n",
    "\n",
    "pd_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Loading (Complete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import os\n",
    "\n",
    "# # Define input folder containing the preprocessed audio files\n",
    "# input_folder = r'AudioSet\\new_created'\n",
    "\n",
    "# # Initialize lists to store data\n",
    "# file_paths = []\n",
    "# class_names = []\n",
    "\n",
    "# # Iterate over each file in the input folder\n",
    "# for file_name in os.listdir(input_folder):\n",
    "#     folder_path = os.path.join(input_folder, file_name)\n",
    "    \n",
    "#     # Check if the item is a directory\n",
    "#     if os.path.isdir(folder_path):\n",
    "#         class_name = file_name  # Use folder name as class name\n",
    "        \n",
    "#         # Iterate over files in the directory\n",
    "#         for file in os.listdir(folder_path):\n",
    "#             if file.endswith(\".wav\"):\n",
    "#                 file_path = os.path.join(file_name, file)  # Changed to only file_name\n",
    "                \n",
    "#                 # Append file path and class name to lists\n",
    "#                 file_paths.append(file_path)\n",
    "#                 class_names.append(class_name)\n",
    "\n",
    "# # Create DataFrame from the collected data\n",
    "# preprocessed_df = pd.DataFrame({\n",
    "#     'filename': file_paths,\n",
    "#     'category': class_names\n",
    "# })\n",
    "\n",
    "# # Display DataFrame\n",
    "# pd_data = preprocessed_df\n",
    "\n",
    "# pd_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_data_path = 'AudioSet\\new_created'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>category</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AudioSet\\new_created\\book_page_flip\\a5-book-fl...</td>\n",
       "      <td>book_page_flip</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AudioSet\\new_created\\book_page_flip\\a5-book-fl...</td>\n",
       "      <td>book_page_flip</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AudioSet\\new_created\\book_page_flip\\a5-book-fl...</td>\n",
       "      <td>book_page_flip</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AudioSet\\new_created\\book_page_flip\\a5-book-fl...</td>\n",
       "      <td>book_page_flip</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AudioSet\\new_created\\book_page_flip\\a5-book-fl...</td>\n",
       "      <td>book_page_flip</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>AudioSet\\new_created\\book_page_flip\\a5-book-fl...</td>\n",
       "      <td>book_page_flip</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>AudioSet\\new_created\\book_page_flip\\a5-book-fl...</td>\n",
       "      <td>book_page_flip</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>AudioSet\\new_created\\book_page_flip\\a5-book-fl...</td>\n",
       "      <td>book_page_flip</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>AudioSet\\new_created\\book_page_flip\\a5-book-fl...</td>\n",
       "      <td>book_page_flip</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>AudioSet\\new_created\\book_page_flip\\a5-book-fl...</td>\n",
       "      <td>book_page_flip</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            filename        category  target\n",
       "0  AudioSet\\new_created\\book_page_flip\\a5-book-fl...  book_page_flip      10\n",
       "1  AudioSet\\new_created\\book_page_flip\\a5-book-fl...  book_page_flip      10\n",
       "2  AudioSet\\new_created\\book_page_flip\\a5-book-fl...  book_page_flip      10\n",
       "3  AudioSet\\new_created\\book_page_flip\\a5-book-fl...  book_page_flip      10\n",
       "4  AudioSet\\new_created\\book_page_flip\\a5-book-fl...  book_page_flip      10\n",
       "5  AudioSet\\new_created\\book_page_flip\\a5-book-fl...  book_page_flip      10\n",
       "6  AudioSet\\new_created\\book_page_flip\\a5-book-fl...  book_page_flip      10\n",
       "7  AudioSet\\new_created\\book_page_flip\\a5-book-fl...  book_page_flip      10\n",
       "8  AudioSet\\new_created\\book_page_flip\\a5-book-fl...  book_page_flip      10\n",
       "9  AudioSet\\new_created\\book_page_flip\\a5-book-fl...  book_page_flip      10"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Your specified classes\n",
    "my_classes = ['Silence', 'Male speech, man speaking', 'Female speech, woman speaking', \n",
    "              'Vacuum cleaner', 'Computer keyboard', 'Clicking', 'Sneeze', \n",
    "              'Cough', 'Laughter', 'Hair dryer', 'book_page_flip']\n",
    "\n",
    "# Mapping classes to IDs\n",
    "map_class_to_id = {'Silence': 0, 'Male speech, man speaking': 1, 'Female speech, woman speaking': 2, \n",
    "                   'Vacuum cleaner': 3, 'Computer keyboard': 4, 'Clicking': 5, 'Sneeze': 6, \n",
    "                   'Cough': 7, 'Laughter': 8, 'Hair dryer': 9, 'book_page_flip': 10}\n",
    "\n",
    "# Filter the data based on specified classes\n",
    "filtered_pd = pd_data[pd_data.category.isin(my_classes)]\n",
    "\n",
    "# Assign IDs to the classes\n",
    "class_id = filtered_pd['category'].apply(lambda name: map_class_to_id[name])\n",
    "filtered_pd = filtered_pd.assign(target=class_id)\n",
    "\n",
    "filtered_pd['filename'] = filtered_pd['filename'].apply(lambda row: os.path.join(\"AudioSet\\\\new_created\", row))\n",
    "\n",
    "# Display the first 10 rows\n",
    "filtered_pd.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the audio files and retrieve embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorSpec(shape=(), dtype=tf.string, name=None),\n",
       " TensorSpec(shape=(), dtype=tf.int64, name=None))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filenames = filtered_pd['filename']\n",
    "targets = filtered_pd['target']\n",
    "\n",
    "main_ds = tf.data.Dataset.from_tensor_slices((filenames, targets))\n",
    "main_ds.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using a while_loop for converting IO>AudioResample\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using a while_loop for converting IO>AudioResample\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(TensorSpec(shape=<unknown>, dtype=tf.float32, name=None),\n",
       " TensorSpec(shape=(), dtype=tf.int64, name=None))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_wav_for_map(filename, label):\n",
    "  return load_wav_16k_mono(filename), label\n",
    "\n",
    "main_ds = main_ds.map(load_wav_for_map)\n",
    "main_ds.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorSpec(shape=(1024,), dtype=tf.float32, name=None),\n",
       " TensorSpec(shape=(), dtype=tf.int64, name=None))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# applies the embedding extraction model to a wav data\n",
    "def extract_embedding(wav_data, label):\n",
    "  ''' run YAMNet to extract embedding from the wav data '''\n",
    "  scores, embeddings, spectrogram = yamnet_model(wav_data)\n",
    "  num_embeddings = tf.shape(embeddings)[0]\n",
    "  return (embeddings,\n",
    "            tf.repeat(label, num_embeddings))\n",
    "            \n",
    "\n",
    "# extract embedding\n",
    "main_ds = main_ds.map(extract_embedding).unbatch()\n",
    "main_ds.element_spec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting to numpy arrays: 60800it [08:04, 125.45it/s]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Convert TensorFlow dataset to numpy arrays\n",
    "X = []\n",
    "y = []\n",
    "for data in tqdm(main_ds.as_numpy_iterator(), desc=\"Converting to numpy arrays\"):\n",
    "    X.append(data[0])\n",
    "    y.append(data[1])\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "# Split the dataset into train and test sets with an 80-20 ratio\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "\n",
    "# Optionally, convert back to TensorFlow datasets if needed\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
    "\n",
    "# Apply necessary preprocessing steps\n",
    "train_ds = train_ds.shuffle(1000).batch(32).cache().prefetch(tf.data.AUTOTUNE)\n",
    "test_ds = test_ds.batch(32).cache().prefetch(tf.data.AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the directory paths to save the datasets\n",
    "# train_ds_dir = \"./train_dataset\"\n",
    "# test_ds_dir = \"./test_dataset\"\n",
    "\n",
    "# # Save the train_ds\n",
    "# tf.data.experimental.save(train_ds, train_ds_dir)\n",
    "\n",
    "# # Save the test_ds\n",
    "# tf.data.experimental.save(test_ds, test_ds_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model FineTuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten (Flatten)           (None, 1024)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 16)                16400     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 16)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 16)                272       \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 16)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 16)                272       \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 11)                187       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 17,131\n",
      "Trainable params: 17,131\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import models, layers\n",
    "\n",
    "\n",
    "my_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(1024), dtype=tf.float32, name='input_embedding'),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(16, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.1),\n",
    "    tf.keras.layers.Dense(16, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.1),\n",
    "    tf.keras.layers.Dense(16, activation='relu'),\n",
    "    tf.keras.layers.Dense(len(my_classes))\n",
    "])\n",
    "\n",
    "\n",
    "my_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                 optimizer=\"adam\",\n",
    "                 metrics=['accuracy'])\n",
    "\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='loss',\n",
    "                                            patience=3,\n",
    "                                            restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1710/1710 [==============================] - 4s 2ms/step - loss: 0.8490 - accuracy: 0.6948 - val_loss: 0.5528 - val_accuracy: 0.7939\n",
      "Epoch 2/20\n",
      "1710/1710 [==============================] - 4s 2ms/step - loss: 0.6053 - accuracy: 0.7757 - val_loss: 0.4927 - val_accuracy: 0.8072\n",
      "Epoch 3/20\n",
      "1710/1710 [==============================] - 3s 2ms/step - loss: 0.5652 - accuracy: 0.7916 - val_loss: 0.4739 - val_accuracy: 0.8266\n",
      "Epoch 4/20\n",
      "1710/1710 [==============================] - 3s 2ms/step - loss: 0.5666 - accuracy: 0.7988 - val_loss: 0.4780 - val_accuracy: 0.8326\n",
      "Epoch 5/20\n",
      "1710/1710 [==============================] - 3s 2ms/step - loss: 0.5512 - accuracy: 0.7997 - val_loss: 0.4621 - val_accuracy: 0.8368\n",
      "Epoch 6/20\n",
      "1710/1710 [==============================] - 3s 2ms/step - loss: 0.5323 - accuracy: 0.8074 - val_loss: 0.4746 - val_accuracy: 0.8153\n",
      "Epoch 7/20\n",
      "1710/1710 [==============================] - 3s 2ms/step - loss: 0.5190 - accuracy: 0.8114 - val_loss: 0.4597 - val_accuracy: 0.8156\n",
      "Epoch 8/20\n",
      "1710/1710 [==============================] - 3s 2ms/step - loss: 0.5187 - accuracy: 0.8114 - val_loss: 0.4784 - val_accuracy: 0.8148\n",
      "Epoch 9/20\n",
      "1710/1710 [==============================] - 3s 2ms/step - loss: 0.5006 - accuracy: 0.8176 - val_loss: 0.4405 - val_accuracy: 0.8438\n",
      "Epoch 10/20\n",
      "1710/1710 [==============================] - 3s 2ms/step - loss: 0.4946 - accuracy: 0.8190 - val_loss: 0.4316 - val_accuracy: 0.8467\n",
      "Epoch 11/20\n",
      "1710/1710 [==============================] - 3s 2ms/step - loss: 0.4847 - accuracy: 0.8216 - val_loss: 0.4358 - val_accuracy: 0.8449\n",
      "Epoch 12/20\n",
      "1710/1710 [==============================] - 4s 2ms/step - loss: 0.4897 - accuracy: 0.8214 - val_loss: 0.4409 - val_accuracy: 0.8439\n",
      "Epoch 13/20\n",
      "1710/1710 [==============================] - 3s 2ms/step - loss: 0.4811 - accuracy: 0.8247 - val_loss: 0.4377 - val_accuracy: 0.8410\n",
      "Epoch 14/20\n",
      "1710/1710 [==============================] - 3s 2ms/step - loss: 0.4749 - accuracy: 0.8264 - val_loss: 0.4220 - val_accuracy: 0.8480\n",
      "Epoch 15/20\n",
      "1710/1710 [==============================] - 4s 3ms/step - loss: 0.4850 - accuracy: 0.8258 - val_loss: 0.4470 - val_accuracy: 0.8426\n",
      "Epoch 16/20\n",
      "1710/1710 [==============================] - 4s 3ms/step - loss: 0.4662 - accuracy: 0.8293 - val_loss: 0.4482 - val_accuracy: 0.8211\n",
      "Epoch 17/20\n",
      "1710/1710 [==============================] - 3s 2ms/step - loss: 0.4837 - accuracy: 0.8251 - val_loss: 0.4244 - val_accuracy: 0.8467\n",
      "Epoch 18/20\n",
      "1710/1710 [==============================] - 4s 2ms/step - loss: 0.4644 - accuracy: 0.8295 - val_loss: 0.4232 - val_accuracy: 0.8459\n",
      "Epoch 19/20\n",
      "1710/1710 [==============================] - 3s 2ms/step - loss: 0.4608 - accuracy: 0.8310 - val_loss: 0.4324 - val_accuracy: 0.8492\n",
      "Epoch 20/20\n",
      "1710/1710 [==============================] - 3s 2ms/step - loss: 0.4628 - accuracy: 0.8296 - val_loss: 0.4281 - val_accuracy: 0.8462\n"
     ]
    }
   ],
   "source": [
    "history = my_model.fit(train_ds,\n",
    "                       epochs=20,\n",
    "                       validation_data=test_ds,\n",
    "                       callbacks=callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "190/190 [==============================] - 0s 1ms/step - loss: 0.4281 - accuracy: 0.8462\n",
      "Loss:  0.42805489897727966\n",
      "Accuracy:  0.8462170958518982\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = my_model.evaluate(test_ds)\n",
    "\n",
    "print(\"Loss: \", loss)\n",
    "print(\"Accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_wav_16k_mono' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m testing_wav_file_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAudioSet\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mnew_created\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mClicking\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mmouse_click_26_aug_comb_PitchShift_TimeStretch_AddGaussianNoise_aug.wav\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 3\u001b[0m testing_wav_data \u001b[38;5;241m=\u001b[39m \u001b[43mload_wav_16k_mono\u001b[49m(testing_wav_file_name)\n\u001b[0;32m      5\u001b[0m _ \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39mplot(testing_wav_data)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Play the audio file.\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'load_wav_16k_mono' is not defined"
     ]
    }
   ],
   "source": [
    "testing_wav_file_name = r'AudioSet\\new_created\\Clicking\\mouse_click_26_aug_comb_PitchShift_TimeStretch_AddGaussianNoise_aug.wav'\n",
    "\n",
    "testing_wav_data = load_wav_16k_mono(testing_wav_file_name)\n",
    "\n",
    "_ = plt.plot(testing_wav_data)\n",
    "\n",
    "# Play the audio file.\n",
    "display.Audio(testing_wav_data, rate=16000)\n",
    "\n",
    "scores, embeddings, spectrogram = yamnet_model(testing_wav_data)\n",
    "result = my_model(embeddings).numpy()\n",
    "\n",
    "inferred_class = my_classes[result.mean(axis=0).argmax()]\n",
    "print(f'The main sound is: {inferred_class}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save a model that can directly take a WAV file as input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReduceMeanLayer(tf.keras.layers.Layer):\n",
    "  def __init__(self, axis=0, **kwargs):\n",
    "    super(ReduceMeanLayer, self).__init__(**kwargs)\n",
    "    self.axis = axis\n",
    "\n",
    "  def call(self, input):\n",
    "    return tf.math.reduce_mean(input, axis=self.axis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./custom_classes_yamnet\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./custom_classes_yamnet\\assets\n"
     ]
    }
   ],
   "source": [
    "saved_model_path = './custom_classes_yamnet'\n",
    "\n",
    "input_segment = tf.keras.layers.Input(shape=(), dtype=tf.float32, name='audio')\n",
    "embedding_extraction_layer = hub.KerasLayer(yamnet_model_handle,\n",
    "                                            trainable=False, name='yamnet')\n",
    "_, embeddings_output, _ = embedding_extraction_layer(input_segment)\n",
    "serving_outputs = my_model(embeddings_output)\n",
    "serving_outputs = ReduceMeanLayer(axis=0, name='classifier')(serving_outputs)\n",
    "serving_model = tf.keras.Model(input_segment, serving_outputs)\n",
    "serving_model.save(saved_model_path, include_optimizer=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.keras.utils.plot_model(serving_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load your saved model to verify that it works as expected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stand-alone Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_model_path = 'custom_classes_yamnet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "reloaded_model = tf.saved_model.load(saved_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And for the final test: given some sound data directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using a while_loop for converting IO>AudioResample\n"
     ]
    }
   ],
   "source": [
    "testing_wav_file_name = r'AudioSet\\new_created\\Clicking\\mouse_click_26_aug_comb_PitchShift_TimeStretch_AddGaussianNoise_aug.wav'\n",
    "\n",
    "testing_wav_data = load_wav_16k_mono(testing_wav_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your specified classes\n",
    "my_classes = ['Silence', 'Male speech, man speaking', 'Female speech, woman speaking', \n",
    "              'Vacuum cleaner', 'Computer keyboard', 'Clicking', 'Sneeze', \n",
    "              'Cough', 'Laughter', 'Hair dryer', 'book_page_flip']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilities:\n",
      "Silence: 0.0000\n",
      "Male speech, man speaking: 0.0000\n",
      "Female speech, woman speaking: 0.0000\n",
      "Vacuum cleaner: 0.0000\n",
      "Computer keyboard: 0.0000\n",
      "Clicking: 0.3376\n",
      "Sneeze: 0.1743\n",
      "Cough: 0.3251\n",
      "Laughter: 0.0713\n",
      "Hair dryer: 0.0000\n",
      "book_page_flip: 0.0918\n",
      "\n",
      "The main sound is: Clicking with probability 0.3376\n"
     ]
    }
   ],
   "source": [
    "reloaded_results = reloaded_model(testing_wav_data)\n",
    "probabilities = tf.nn.softmax(reloaded_results).numpy()\n",
    "predicted_class_index = tf.math.argmax(reloaded_results).numpy()\n",
    "predicted_class = my_classes[predicted_class_index]\n",
    "\n",
    "print(\"Probabilities:\")\n",
    "for i, prob in enumerate(probabilities):\n",
    "    print(f\"{my_classes[i]}: {prob:.4f}\")\n",
    "\n",
    "print(f'\\nThe main sound is: {predicted_class} with probability {probabilities[predicted_class_index]:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to try your new model on a serving setup, you can use the 'serving_default' signature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The main sound is: Clicking\n"
     ]
    }
   ],
   "source": [
    "serving_results = reloaded_model.signatures['serving_default'](testing_wav_data)\n",
    "predicting_1 = my_classes[tf.math.argmax(serving_results['classifier'])]\n",
    "print(f'The main sound is: {predicting_1}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
